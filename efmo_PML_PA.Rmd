---
title: "Practical Machine Learning PA1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

This report describes a method that uses wearable sensor data to predict types of motion. Participants with little to no weightlifting experience were shown how to perform barbell lifts correctly and incorrectly in five different ways. The data comes from accelerometers attached to the belt, forearm, arm, and dumbell of the participants.

The original experiment description and data can be found here: <http://groupware.les.inf.puc-rio.br/har>


##Loading Data and First Look
To begin, a training and testing dataset are loaded from csv files. The training set contains almost twenty thousand observations of 160 variables, one of which describes the type of barbell lift (identified as A, B, C, D, or E). The testing set contains twenty observations with an identical set of variables excluding the movement type. The test set is left unexamined until a final machine learning model is complete, but all data transformations made to the training set are applied to the test set.

```{r}
library(caret)
train <- read.csv("pml-training.csv", header = TRUE)
test <- read.csv("pml-testing.csv", header = TRUE)
```

##Preprocessing
The goal of this stage was to reduce the number of variables in the training dataset in order to increase model training speed and accuracy and reduce noise. 
Near-zero values are identified using the `nzv()` function from the caret package. This reduces the variable count from 160 to 100.

```{r}
nzvAll <- nzv(train)
train2 <- train[,-nzvAll]
test2 <- test[,-nzvAll]
```

Upon further investigation the dataset appears to contain large swaths of NA values. The number of NAs per column is so large that imputing data isn't a realistic option. These columns are identified and removed. This reduces the variable count from 100 to 59. Sadly, this suggests that a great deal of potentially useful sensor data was lost or corrupted.

```{r}
naCols <- colSums(is.na(train2))
quantile(naCols) # a quick look at how many NAs per column there are overall
train3 <- train2[,-which(naCols > 0)]
test3 <- test2[,-which(naCols > 0)]
```

The next steps involve a closer look at the data breakdown by user. The call to `nzv()` identified near-zero variables across the dataset, but a closer look at the dataset user by user reveals that some users have holes in their individual datasets. To keep the entire model consistent, any variables that aren't statistically relevant (as decided by `nzv()`) across all participants are dropped. This reduces the variable count 6 more to 53.

```{r}
vecList <- unlist(lapply(split(train3, train3$user_name), nzv))
nzvByUser <- sort(unique(vecList))
nzvByUser <- nzvByUser[2:length(nzvByUser)]
train4 <- train3[,-nzvByUser]
test4 <- test3[,-nzvByUser]
```

The dataset includes time based variables like dates, timestamps, and time windows. There was no clear organization or description of these variables and I couldn't determine a strong correlation between them and other variables in the dataset. Furthermore, each of the twenty observations in the test dataset are to be treated independently, so without a better understanding of the time variables there isn't much lost by dropping them. This reduces the variable count to 48.

```{r}
train5 <- train4[,c(2,7:53)]
test5 <- test4[,c(2,7:53)]
```
I considered removing the `train5$user_name` variable in order to keep the learning model blind to the user's identity and hopefully more accurate for new users. I ultimately decided to retain the participant's name for two reasons. First, the test set included user names, all of which existed in the training set. Second, after a bit of peeking around the variable relationships as broken down by user name, I decided that there was too much distinct correlation within each user's dataset that could be lost by eliminating the user name data. 
The tight groupings of some example variable pairs in the plots below support this theory. The top row shows two variable pairs colored by user. The bottom row shows the same variable pairs of a single user (Charles) colored by movement type. There seemed to be enough distinct variability in the data according to user to justify including the user name.

```{r}
library(gridExtra)
t5Charles <- train5[which(train5$user_name == "charles"),]
plot1 <- ggplot(train5, aes(x=roll_belt, y=pitch_belt, color = user_name)) + geom_point()
plot2 <- ggplot(train5, aes(x=accel_belt_x, y=accel_belt_z, color = user_name)) + geom_point()
plot3 <- ggplot(t5Charles, aes(x=roll_belt, y=pitch_belt, color = classe)) + geom_point()
plot4 <- ggplot(t5Charles, aes(x=accel_belt_x, y=accel_belt_z,  color = classe)) + geom_point()
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

##Intuition and Model Choice

I randomly assigned 75% of the training set for training and cross validation and the remaining 25% as a test set. The `trainControl` parameter for `train()` in the caret package is customized to perform 5-fold cross validation repeatedly within each iteration of training (see the documentation page <http://topepo.github.io/caret/training.html> for more details).
```{r}
tmpTrain <- createDataPartition(train5$classe, p = 0.75, list = FALSE)
fitControl <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5)
```

I decided to train four different models. Each was a combination of using or not using participant names and using or not using principal component analysis. I was curious to see what the speed and accuracy impact would be for each of these variations and wanted to get a sense for which transformations were helpful and which were not. I used a simple time difference benchmark to get a rough idea of runtime.
The suffix NN or WN denotes 'no username' and 'with username' respectively. The presence of a 'P' in the suffix indicates the model used PCA. As a side note, PCA reduced the variable count from 48 to 20 using the default variance threshold of 0.95.

```{r}
#train a non-PCA model without username cols
trainNN <- train5[,2:48]
tmp <- Sys.time()
modFitNN <- train(classe~., data = trainNN[tmpTrain,], method = "rf", trControl = fitControl)
tdifNN <- Sys.time() - tmp
cmNN <- confusionMatrix(trainNN$classe[-tmpTrain], predict(modFitNN,trainNN[-tmpTrain,]))
predNN <- predict(modFitNN,test5[,2:48])

#train a non-PCA model with username cols
dmyNames <- dummyVars("~ user_name", data = train5)
trsf <- data.frame(predict(dmyNames,train5))
train6 <- cbind(trsf, train5[,2:48])
dmyNames2 <- dummyVars("~ user_name", data = test5)
trsf2 <- data.frame(predict(dmyNames2,test5))
test6 <- cbind(trsf2, test5[,2:48])

tmp <- Sys.time()
modFitWN <- train(classe~., data = train6[tmpTrain,], method = "rf", trControl = fitControl)
tdifWN <- Sys.time() - tmp
cmWN <- confusionMatrix(train6$classe[-tmpTrain], predict(modFitWN,train6[-tmpTrain,]))
predWN <- predict(modFitWN,test6)

#train a PCA model without username cols
ppNN <- preProcess(train5[tmpTrain,2:47],method="pca")
trainPNN <- predict(ppNN,train5[tmpTrain,2:47])
trtestPNN <- predict(ppNN,train5[-tmpTrain,2:47])

predTestPNN <- predict(ppNN,test5[,2:47])
tmp <- Sys.time()
modPNN <- train(train5$classe[tmpTrain]~., method = "rf", data = trainPNN, trControl = fitControl)
tdifPNN <- Sys.time() - tmp
cmPNN <- confusionMatrix(train5$classe[-tmpTrain],predict(modPNN,trtestPNN))
predPNN <- predict(modPNN,predTestPNN)

#train a PCA model with username cols
ppWN <- preProcess(train6[tmpTrain,-53],method="pca")
trainPWN <- predict(ppWN,train6[tmpTrain,-53])
trtestPWN <- predict(ppWN,train6[-tmpTrain,-53])

predTestPWN <- predict(ppWN,test6[,-53])
tmp <- Sys.time()
modPWN <- train(train6$classe[tmpTrain] ~.,method = "rf", data = trainPWN, trControl = fitControl)
tdifPWN <- Sys.time() - tmp
cmPWN <- confusionMatrix(train6$classe[-tmpTrain],predict(modPWN,trtestPWN))
predPWN <- predict(modPWN, predTestPWN)
```

##Out-of-sample Error and Cross-Validation

A comparison of model accuracy using the 25% test set shows that for non-PCA models the runtime increased approximately 200%.

```{r}
tdifNN
tdifPNN
tdifWN
tdifPWN
```

The non-PCA model with user names was about 5% faster than the non-PCA model without usernames. This could be due to variance in converging time within the random forest model. It's also possible that the user name data helped the random forests converge a little faster. 

Both PCA models ran in about 20 minutes. An interesting side note is that the PCA model with user names was faster by a few seconds. After examining the two PCA transformed training sets I realized they both had 20 principal components. This combined with the accuracy results suggests to me that user names provide much less useful information than I originally thought.

Accuracy and error estimates were reported using `confusionMatrix()`.

```{r}
cmNN$overall
cmPNN$overall
cmWN$overall
cmPWN$overall
```

The non-PCA with user names resulted in an accuracy of 99%, the highest of all four models. This is no surprise since this model contained more data than any of the others. The PCA models both reported an accuracy of about 96%, roughly 3 percentage points below their counterpart non-PCA models.


```{r}
allPred = cbind(predNN,predPNN,predWN,predPWN)
allPred
```

The non-PCA models generated equivalent predictions. The PCA model with user names differed from the non-PCA models' prediction for test element 11. The PCA model without user names differed on element 11 in the same way and additionally differed on element 3.

##Final Model and Performance Analysis

I decided to submit the predictions from the non-PCA models. I would have preferred to see the PCA models agree with the non-PCA models and feel more confident in my ability to speed up runtime with PCA, but there was a little too much difference between non-PCA and PCA models. As it turns out I submitted the non-PCA results first and had all correct answers. 

```{r, eval=FALSE}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("predictions/problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(predNN)
```


## Conclusions and Feedback

Final model performance using actual test values as a benchmark:
100% - non-PCA with user names
100% - non-PCA without user names
95% - PCA with user names
90% - PCA without user names

This accuracy order also matches the models ordered by the amount of data they were trained with, greatest to least, so these results appear to make sense.

I'm happy with my results and am eager to peer review other methods. Please leave your feedback and critique of my model! I would love to know what the rest of the class thinks of my methods and how they could be improved. Thank you!  